# ML-ALS

### 前提

ALS 的核心假设是：打分矩阵A是近似低秩的，即一个$m∗n$ 的打分矩阵 A可以用两个小矩阵$U(m∗k)$ 和$V(n∗k)$ 的乘积来近似：$A≈UV^T$  , $k<<m,n$  

如果评分理解成相似度，那么评分矩阵$A(m∗n)$ 就可以由用户喜好特征矩阵$U(m∗k)$ 和产品特征矩阵$V(n∗k)$ 的乘积。



### 损失函数

我们使用用户喜好特征矩阵$U(m∗k)$ 中的第$i$ 个用户的特征向量$u_i$ ，和产品特征矩阵$V(n∗k)$ 第$j$ 个产品的特征向量$v_j$ 来预测打分矩阵$A(m∗n)$ 中的 $a_{ij}$  。我们可以得出一下的矩阵分解模型的损失函数为：

​							$$ C=∑_{(i,j)∈R}[(a_{ij}−u_iv^T_j)^2+λ(u^2_i+v^2_j)] $$



有了损失函数之后，下面就开始介绍优化方法。通常的优化方法分为两种：

* 交叉最小二乘法（alternative least squares）
* 随机梯度下降法（stochastic gradient descent）。



### ALS算法原理

ALS算法的思想就是：先随机生成然后固定它求解，再固定求解，这样交替进行下去，直到取得最优解$min(C)$ 。因为每步迭代都会降低误差，并且误差是有下界的，所以 ALS 一定会收敛。但由于问题是非凸的，ALS 并不保证会收敛到全局最优解。但在实际应用中，ALS 对初始点不是很敏感，是否全局最优解造成的影响并不大。

算法的执行步骤：
先随机生成一个。一般可以取0值或者全局均值。
固定，即认为是已知的常量，来求解： 
C=∑(i,j)∈R[(aij−u(0)ivTj)2+λ((u2i)(0)+v2j)]

由于上式中只有vj
一个未知变量，因此C的最优化问题转化为最小二乘问题，用最小二乘法求解vj
的最优解： 
固定j,j∈(1,2,...,n)
，则：等式两边关于为vj
求导得：
d(c)d(vj)

=dd(vj)(∑i=1m[(aij−u(0)ivTj)2+λ((u2i)(0)+v2j)])

=∑i=1m[2(aij−u(0)ivTj)(−(uTi)(0))+2λvj]

=2∑i=1m[(u(0)i(uTi)(0)+λ)vj−aij(uTi)(0)]
令d(c)d(vj)=0
，可得： 
∑i=1m[(u(0)i(uTi)(0)+λ)vj]=∑i=1maij(uTi)(0)

=>(U(0)(UT)(0)+λE)vj=aTjU(0)
令 M1=U(0)(UT)(0)+λE,M2=aTjU(0)
，则vj=M−11M2

按照上式依次计算v1，v2，...，vn
，从而得到V(0)
同理，用步骤2中类似的方法: 
C=∑(i,j)∈R[(aij−ui(vTj)(0))2+λ(u2i+(v2j)(0))]

固定i,i∈(1,2,...,m)
，则：等式两边关于为ui
求导得：
d(c)d(ui)

=dd(ui)(∑j=1n[(aij−ui(vTj)(0))2+λ((u2i)+(v2j)(0))])

=∑j=1n[2(aij−ui(vTj)(0))(−(vTj)(0))+2λui]

=2∑j=1n[(v(0)j(vTj)(0)+λ)ui−aij(vTj)(0)]
令d(c)d(ui)=0
，可得： 
∑j=1n[(v(0)j(vTj)(0)+λ)ui]=∑j=1naij(vTj)(0)

=>((V(0)(VT)(0)+λE)ui=aTiV(0)

令 M1=V(0)(VT)(0)+λE,M2=aTiV(0)
，则ui=M−11M2

按照上式依次计算u1，u2，...，un
，从而得到U(1)
循环执行步骤2、3，直到损失函数C的值收敛（或者设置一个迭代次数N，迭代执行步骤2、3，N次后停止）。这样，就得到了C最优解对应的矩阵U、V。


